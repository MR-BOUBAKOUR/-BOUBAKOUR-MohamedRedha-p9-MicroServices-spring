# Performance analysis

## ğŸ§ª Performance test scope and limitations

These performance tests were conducted under the following conditions:

- All components (application, observability stack, and load testing tools) were deployed and run on the same local machine
- Simulated access to a random patient record based on a patient ID generated by the tests (for the Gateway).
- Direct targeting of the endpoint triggering an assessment for the target patient, bypassing the Gateway by exposing the Assessment microservice externally.
- Direct targeting of the endpoint retrieving data for the target patient, bypassing the Gateway by exposing the Patients microservice externally.

**Notably, these tests did not include:**

- Patient creation or note creation operations.
- Triggering of events by assessments that would be captured by RabbitMQ and forwarded to the Notifications microservice.

The goal of this exercise was primarily to isolate and understand the root cause of the bottleneck.

> If you have feedback, ideas, or a different interpretation of the results, I would be genuinely grateful to hear it. I approach this analysis as a learning opportunity and would greatly appreciate any expert insights that could improve my investigation or methodology.

---

## ğŸ“ Initial symptom

- **Problem**: performance degradation beyond specific load threshold
- **Observation**: Gateway + individual services performing well, but system saturated
- **Intuition**: reactive Spring Cloud Gateway as bottleneck

---

## ğŸšª Initial hypothesis: Gateway bottleneck

### Gateway optimization attempt

- **Netty configuration optimization (pools, threads)**: no substantial improvement
- **token caching + log level change everywhere (`DEBUG` â†’ `INFO`)**: meaningful improvement but the same observed ceiling (CPU at 95-100%)

### Investigating beyond the Gateway: isolated service testing

- **Approach**: test each microservice individually, including their own downstream calls
- **Observation**: each component - Gateway, Assessment, and Patients microservices - independently reaches 100% CPU under load
- **Key discovery**: 
  - performance issues persist even when bypassing the Gateway

## ğŸšª Second hypothesis: environment/hardware limitation

### Investigating beyond the application: environment stabilization

- **Initial observation**: significant difference in test results when comparing runs with and without the observability stack
- **Approach**: the environment was carefully reset before each run :
  - Containers and volumes fully removed before each test start
  - Smoke tests executed to confirm service readiness
- **Key discovery**: after stabilization, the observability stack had no significant impact on response times or throughput.

---

## ğŸ§  Root cause analysis

### ğŸ¯ Primary cause
**Hardware resource contention**  
The main performance bottleneck stems from the application, observability stack, and load testing tool running concurrently on the same machine, leading to CPU saturation. This is an environmental limitation, not an application-level issue.

### ğŸ” Secondary factor
**Lack of environment reset**  
Initial test results were inconsistent due to leftover state from previous runs. After properly resetting the environment between tests, results became stable and reliable.

---

## ğŸ’¡ Key conclusions

- Microservices architecture is performant and stable under load
- Observability stack overhead exists but remains minimal when:
  - the number of virtual users is moderate 
  - the environment is clean and properly isolated
- Proper environment reset and test isolation are critical for reliable performance testing
- For realistic production performance testing and to avoid resource contention, the application, load generation, and observability stack should run on separate, isolated environments.